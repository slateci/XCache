{
    "docs": [
        {
            "location": "/",
            "text": "XCache\n\n\nWhat is it?\n\n\nXCache is a service that provides caching of data accessed using \nxrootd protocol\n. It sits in between client and an upstream xrootd servers and can cache/prefetch full files or only blocks already requested. To use it to cache file that is at \n\nroot://origin.org:1094/my_file.txt\n\nsimply prepend name of the caching server:\n\nroot://caching_server.org:1094//root://origin.org:1094/my_file.txt\n  \n\nWhen deployed using kubernetes all relevant parameters are configured throught the container environment variables.\n\n\nLinks\n\n\n\n\nDocker\n\n\nGitHub\n\n\nDocumentation\n\n\nMonitoring\n\n\n\n\nConfiguration\n\n\nMandatory variables\n\n\n\n\n\n\n\n\nVariable\n\n\nMeaning\n\n\n\n\n\n\n\n\n\n\nXC_SITE\n\n\nATLAS specific. Used in registering cache in AGIS and in all monitoring. Has to correspond to ATLAS sitename.\n\n\n\n\n\n\n\n\nOptional settings\n\n\n\n\n\n\n\n\nImplemented\n\n\nOption\n\n\nVariable\n\n\nDefault\n\n\nMeaning\n\n\n\n\n\n\n\n\n\n\nNo\n\n\nRucio N2N\n\n\nXC_RUCIO_N2N\n\n\nTrue\n\n\nThis is ATLAS specific thing. To avoid multiple cache copies of the same file (obtained from different sources) it will strip source specific part of the path.\n\n\n\n\n\n\nYes\n\n\nMonitoring\n\n\nXC_REPORT_COLLECTOR\n\n\nhttp://uct2-collectd.mwt2.org:8080\n\n\nThis is xrootd internal monitoring info. Actual service status is monitored through the kubernetes infrastructure.\n\n\n\n\n\n\nYes\n\n\nPort\n\n\nXC_PORT\n\n\n1094\n\n\n\n\n\n\n\n\nNo\n\n\nSubfile caching\n\n\nSUBFC\n\n\nTrue\n\n\n\n\n\n\n\n\nYes\n\n\nPrefetching\n\n\nXC_PREFETCH\n\n\n0\n\n\n\n\n\n\n\n\nYes\n\n\nBlock size\n\n\nXC_BLOCKSIZE\n\n\n1M\n\n\n\n\n\n\n\n\nYes\n\n\nDisk usage high watermark\n\n\nXC_SPACE_HIGH_WM\n\n\n95%\n\n\n\n\n\n\n\n\nYes\n\n\nDisk usage low watermark\n\n\nXC_SPACE_LOW_WM\n\n\n80%\n\n\n\n\n\n\n\n\nYes\n\n\nRAM size\n\n\nXC_RAMSIZE\n\n\nhalf of the free RAM\n\n\nAt least 1g. Units are ...\n\n\n\n\n\n\n\n\nTo Do\n\n\n\n\nAdd AGIS updating secreet. \n\n\nAdd subfile caching option\n\n\nTest\n\n\nCreate core collecting pod.\n\n\nAdd summary stream monitoring",
            "title": "Home"
        },
        {
            "location": "/#xcache",
            "text": "",
            "title": "XCache"
        },
        {
            "location": "/#what-is-it",
            "text": "XCache is a service that provides caching of data accessed using  xrootd protocol . It sits in between client and an upstream xrootd servers and can cache/prefetch full files or only blocks already requested. To use it to cache file that is at  root://origin.org:1094/my_file.txt \nsimply prepend name of the caching server: root://caching_server.org:1094//root://origin.org:1094/my_file.txt    \nWhen deployed using kubernetes all relevant parameters are configured throught the container environment variables.",
            "title": "What is it?"
        },
        {
            "location": "/#links",
            "text": "Docker  GitHub  Documentation  Monitoring",
            "title": "Links"
        },
        {
            "location": "/#configuration",
            "text": "",
            "title": "Configuration"
        },
        {
            "location": "/#mandatory-variables",
            "text": "Variable  Meaning      XC_SITE  ATLAS specific. Used in registering cache in AGIS and in all monitoring. Has to correspond to ATLAS sitename.",
            "title": "Mandatory variables"
        },
        {
            "location": "/#optional-settings",
            "text": "Implemented  Option  Variable  Default  Meaning      No  Rucio N2N  XC_RUCIO_N2N  True  This is ATLAS specific thing. To avoid multiple cache copies of the same file (obtained from different sources) it will strip source specific part of the path.    Yes  Monitoring  XC_REPORT_COLLECTOR  http://uct2-collectd.mwt2.org:8080  This is xrootd internal monitoring info. Actual service status is monitored through the kubernetes infrastructure.    Yes  Port  XC_PORT  1094     No  Subfile caching  SUBFC  True     Yes  Prefetching  XC_PREFETCH  0     Yes  Block size  XC_BLOCKSIZE  1M     Yes  Disk usage high watermark  XC_SPACE_HIGH_WM  95%     Yes  Disk usage low watermark  XC_SPACE_LOW_WM  80%     Yes  RAM size  XC_RAMSIZE  half of the free RAM  At least 1g. Units are ...",
            "title": "Optional settings"
        },
        {
            "location": "/#to-do",
            "text": "Add AGIS updating secreet.   Add subfile caching option  Test  Create core collecting pod.  Add summary stream monitoring",
            "title": "To Do"
        },
        {
            "location": "/instructions/",
            "text": "Creating secrets\n\n\nFirst one needs to create k8s secret that contains robot certificate that will be used by the XCache to get the data from origin servers. \nCreate directory \ncertificates\n in \nkube\n folder. In it add files \nxcache.key.pem\n and \nxcache.crt.pem\n.\nThan execute file \nxcache-secret.bat\n. This is normally done only once. Upon creation, you may delete \ncertificates\n folder.\n\n\nCreate service\n\n\nThis service maps xrootd port to external ports. The only thing to be changed is the last line of the \nxcache_service.yaml\n file. It has to point to the node's IP address.\nThis is normally done only once. \n\n\nFrom kube directory do:\n\n\nkubectl create -f xcache_service.yaml\n\n\n\n\nConfigure and start XCache\n\n\nThere are several settings that should be set before starting XCache:\n\n\n\n\ncache directory - replace \n/scratch\n with directory that should be used to store cached data.\n\n\n\n\n - name: xcache-data\n   hostPath:\n     path: /scratch\n\n\n\n\n\n\nall the parameters that start with \nXC_\n\n\n\n\nOnce everything is set up, to start the service simply do:\n\n\nkubectl create -f xcache.yaml\n\n\n\n\nRun stress test\n\n\nThe stress test k8s pod runs a simple client that repeatedly transfers (using xrdcp) 26 5GB files through the XCache. Files are stored at AGLT2. The only configuration needed is in line: \nargs: [\"root://xcache.mwt2.org:1094\",\"MWT2\"]\n where the first argument has to be changed to address of the xcache service you are testing. Then to start the test do:\n\n\nkubectl create -f xcache-stress_test.yaml\n\n\n\n\nTo get test's logs do:\n\n\nkubectl logs stress-test -c stresser\n\n\n\n\nDocker (obsolete)\n\n\nTo start:\n\n\ndocker run -d \\\n-e XC_SPACE_HIGH_WM='0.95' \\\n-e XC_SPACE_LOW_WM='0.80' \\\n-e XC_PORT='1094' \\\n-e XC_RAMSIZE='1g' \\\n-e XC_BLOCKSIZE='1M' \\\n-e XC_PREFETCH='0' \\\n-p 1094:1094 \\\n-v /root/xcache_test/vomsdir:/etc/grid-security/vomsdir/ \\\n-v /root/xcache_test/proxy:/etc/grid-security/ \\\n--name xCache slateci/xcache\n\n\n\n\nTo log into it and check logs:\n\n\ndocker exec -it xCache bash\nls /data/xrd/var/log/\n\nTo stop it:\n\n\ndocker stop xCache\n\n\ndocker rm xCache\n\n\nTo update it:\n\ndocker pull slateci/xcache",
            "title": "Instructions"
        },
        {
            "location": "/instructions/#creating-secrets",
            "text": "First one needs to create k8s secret that contains robot certificate that will be used by the XCache to get the data from origin servers. \nCreate directory  certificates  in  kube  folder. In it add files  xcache.key.pem  and  xcache.crt.pem .\nThan execute file  xcache-secret.bat . This is normally done only once. Upon creation, you may delete  certificates  folder.",
            "title": "Creating secrets"
        },
        {
            "location": "/instructions/#create-service",
            "text": "This service maps xrootd port to external ports. The only thing to be changed is the last line of the  xcache_service.yaml  file. It has to point to the node's IP address.\nThis is normally done only once.   From kube directory do:  kubectl create -f xcache_service.yaml",
            "title": "Create service"
        },
        {
            "location": "/instructions/#configure-and-start-xcache",
            "text": "There are several settings that should be set before starting XCache:   cache directory - replace  /scratch  with directory that should be used to store cached data.    - name: xcache-data\n   hostPath:\n     path: /scratch   all the parameters that start with  XC_   Once everything is set up, to start the service simply do:  kubectl create -f xcache.yaml",
            "title": "Configure and start XCache"
        },
        {
            "location": "/instructions/#run-stress-test",
            "text": "The stress test k8s pod runs a simple client that repeatedly transfers (using xrdcp) 26 5GB files through the XCache. Files are stored at AGLT2. The only configuration needed is in line:  args: [\"root://xcache.mwt2.org:1094\",\"MWT2\"]  where the first argument has to be changed to address of the xcache service you are testing. Then to start the test do:  kubectl create -f xcache-stress_test.yaml  To get test's logs do:  kubectl logs stress-test -c stresser",
            "title": "Run stress test"
        },
        {
            "location": "/instructions/#docker-obsolete",
            "text": "To start:  docker run -d \\\n-e XC_SPACE_HIGH_WM='0.95' \\\n-e XC_SPACE_LOW_WM='0.80' \\\n-e XC_PORT='1094' \\\n-e XC_RAMSIZE='1g' \\\n-e XC_BLOCKSIZE='1M' \\\n-e XC_PREFETCH='0' \\\n-p 1094:1094 \\\n-v /root/xcache_test/vomsdir:/etc/grid-security/vomsdir/ \\\n-v /root/xcache_test/proxy:/etc/grid-security/ \\\n--name xCache slateci/xcache  To log into it and check logs:  docker exec -it xCache bash\nls /data/xrd/var/log/ \nTo stop it:  docker stop xCache  docker rm xCache  To update it: docker pull slateci/xcache",
            "title": "Docker (obsolete)"
        },
        {
            "location": "/k8s-deployment/",
            "text": "Kubernetes v1.10 on CentOS 7\n\n\nHead node installation\n\n\nOn the head node:\n\n\n# yum update -y\n# yum install -y docker\n# systemctl enable docker && systemctl start docker\n# cat <<EOF > /etc/yum.repos.d/kubernetes.repo\n[kubernetes]\nname=Kubernetes\nbaseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-\\$basearch\nenabled=1\ngpgcheck=1\nrepo_gpgcheck=1\ngpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg\nEOF\n# setenforce 0\n# yum install -y kubelet kubeadm kubectl\n# systemctl enable kubelet && systemctl start kubelet\n\n\n\n\nAt this point, docker should be running and the kubelet should be crash looping. if Docker won't start and complains about overlayfs, make sure you've updated and reboot into the latest kernel. \n\n\nNow start kubeadm\n\n\n# kubeadm init\n\n\n\n\nOnce it's done, you should see:\n\n\nYour Kubernetes master has initialized successfully!\n\n\n\n\nFollow the instructions from the shell output. \nmost importantly\n, save the \nkubeadm join ...\n  line as it has an important token in it. This is needed to join additional nodes to the cluster. \n\n\nAdd Calico networking (there are many possible plugins, we've chosen this one based on best practices)\n\n\n# kubectl apply -f https://docs.projectcalico.org/v3.0/getting-started/kubernetes/installation/hosted/kubeadm/1.7/calico.yaml\n\n\n\n\nCheck to see that the Master is ready\n\n\n$ kubectl get nodes\nNAME                STATUS    ROLES     AGE       VERSION\nk8s-head.mwt2.org   Ready     master    1h        v1.10.0\n\n\n\n\n\nCreate a .kube/config for a regular user:\n\n\n$ mkdir -p $HOME/.kube\n$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n$ sudo chown $(id -u):$(id -g) $HOME/.kube/config\n\n\n\n\nOptional - Enabling the Kubernetes dashboard\n\n\n$ kubectl create -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml\nsecret \"kubernetes-dashboard-certs\" created\nserviceaccount \"kubernetes-dashboard\" created\nrole.rbac.authorization.k8s.io \"kubernetes-dashboard-minimal\" created\nrolebinding.rbac.authorization.k8s.io \"kubernetes-dashboard-minimal\" created\ndeployment.apps \"kubernetes-dashboard\" created\nservice \"kubernetes-dashboard\" created\n\n\n\n\nTo access the dashboard, you'll want to have your \n.kube/config\n and  \nkubectl\n on your workstation/laptop and run\n\n\nkubectl proxy\n\n\n\n\nThen you can access the dashboard at \nhttp://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/\n\n\nIt may display errors about not being able to work in the default namespace. If so, you'll need to do the following:\n\n\nkubectl create serviceaccount dashboard -n default\nkubectl create clusterrolebinding dashboard-admin -n default --clusterrole=cluster-admin --serviceaccount=default:dashboard\n\n\n\n\nThen, to get the token for authentication:\n\n\nkubectl get secret $(./kubectl get serviceaccount dashboard -o jsonpath=\"{.secrets[0].name}\") -o jsonpath=\"{.data.token}\" | base64 --decode\n\n\n\n\nOptional - Allow workers to run on head node\n\n\nYou can now add additional nodes to the cluster, or \"taint\" the master to allow work to be scheduled onto it (single node configuration)\n\n\nkubectl taint nodes --all node-role.kubernetes.io/master-\n\n\n\n\nAdding Workers\n\n\nHopefully you saved the output of \nkubeadm init\n. If so, you can just copy/paste the \nkubeadm join ...\n bits from the output into your worker node.\n\n\nIf not, here's how to regenerate it. On the master:\n\n\n$ sudo kubeadm token create --print-join-command\nkubeadm join <master ip>:6443 --token <token> --discovery-token-ca-cert-hash sha256:<hash>\n\n\n\n\nJust paste that into your worker. \nNote that the token expires after 24h, after which you'll need to create a new token to add workers to your cluster\n. \n\n\n\n\nAdding namespaces, users, and access control\n\n\nFor all of the following, it's assumed that you'll save the yaml to a file and run \nkubectl -f\n against it.\n\n\nCreating a new namespace\n\n\nIt's helpful to have users isolated to a separate namespace. Here's how to create one:\n\n\nkind: Namespace\napiVersion: v1\nmetadata:\n    name: development\n    labels:\n        name: development\n\n\n\n\nCreate a new user context\n\n\nThe easiest way to add new users is to use kubeadm to generate a new client context:\n\n\n# kubeadm alpha phase kubeconfig user --client-name=lincoln\napiVersion: v1\nclusters:\n- cluster:\n    certificate-authority-data: <base64-encoded data> \n    server: https://<your k8s API IP>:6443\n  name: kubernetes\ncontexts:\n- context:\n    cluster: kubernetes\n    user: lincoln\n  name: lincoln@kubernetes\ncurrent-context: lincoln@kubernetes\nkind: Config\npreferences: {}\nusers:\n- name: lincoln\n  user:\n    client-certificate-data: <base64 encoded data> \n    client-key-data: <base64 encoded data> \n\n\n\n\nThis is a \n.kube/config\n file that you should send to your non-admin user.\n\n\nCreate roles and bind them to the user\n\n\nKubernetes uses a role-based access control (RBAC) system. To let users do anything, you'll need to \"bind\" a role to them.\n\n\nHere's a role that allows a user to create deployments, replicasets, pods, and jobs:\n\n\nkind: Role\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  namespace: development\n  name: deployment-manager\nrules:\n- apiGroups: [\"\", \"extensions\", \"apps\", \"batch\"]\n  resources: [\"deployments\", \"replicasets\", \"pods\", \"jobs\"]\n  verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\", \"delete\"]\n\n\n\n\nAnd then to associate that role to our newly created user:\n\n\nkind: RoleBinding\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: deployment-manager-binding\n  namespace: development\nsubjects:\n- kind: User\n  name: lincoln\n  apiGroup: \"\"\nroleRef:\n  kind: Role\n  name: deployment-manager\n  apiGroup: \"\"",
            "title": "Setting up k8s cluster"
        },
        {
            "location": "/k8s-deployment/#kubernetes-v110-on-centos-7",
            "text": "",
            "title": "Kubernetes v1.10 on CentOS 7"
        },
        {
            "location": "/k8s-deployment/#head-node-installation",
            "text": "On the head node:  # yum update -y\n# yum install -y docker\n# systemctl enable docker && systemctl start docker\n# cat <<EOF > /etc/yum.repos.d/kubernetes.repo\n[kubernetes]\nname=Kubernetes\nbaseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-\\$basearch\nenabled=1\ngpgcheck=1\nrepo_gpgcheck=1\ngpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg\nEOF\n# setenforce 0\n# yum install -y kubelet kubeadm kubectl\n# systemctl enable kubelet && systemctl start kubelet  At this point, docker should be running and the kubelet should be crash looping. if Docker won't start and complains about overlayfs, make sure you've updated and reboot into the latest kernel.   Now start kubeadm  # kubeadm init  Once it's done, you should see:  Your Kubernetes master has initialized successfully!  Follow the instructions from the shell output.  most importantly , save the  kubeadm join ...   line as it has an important token in it. This is needed to join additional nodes to the cluster.   Add Calico networking (there are many possible plugins, we've chosen this one based on best practices)  # kubectl apply -f https://docs.projectcalico.org/v3.0/getting-started/kubernetes/installation/hosted/kubeadm/1.7/calico.yaml  Check to see that the Master is ready  $ kubectl get nodes\nNAME                STATUS    ROLES     AGE       VERSION\nk8s-head.mwt2.org   Ready     master    1h        v1.10.0  Create a .kube/config for a regular user:  $ mkdir -p $HOME/.kube\n$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n$ sudo chown $(id -u):$(id -g) $HOME/.kube/config",
            "title": "Head node installation"
        },
        {
            "location": "/k8s-deployment/#optional-enabling-the-kubernetes-dashboard",
            "text": "$ kubectl create -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml\nsecret \"kubernetes-dashboard-certs\" created\nserviceaccount \"kubernetes-dashboard\" created\nrole.rbac.authorization.k8s.io \"kubernetes-dashboard-minimal\" created\nrolebinding.rbac.authorization.k8s.io \"kubernetes-dashboard-minimal\" created\ndeployment.apps \"kubernetes-dashboard\" created\nservice \"kubernetes-dashboard\" created  To access the dashboard, you'll want to have your  .kube/config  and   kubectl  on your workstation/laptop and run  kubectl proxy  Then you can access the dashboard at  http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/  It may display errors about not being able to work in the default namespace. If so, you'll need to do the following:  kubectl create serviceaccount dashboard -n default\nkubectl create clusterrolebinding dashboard-admin -n default --clusterrole=cluster-admin --serviceaccount=default:dashboard  Then, to get the token for authentication:  kubectl get secret $(./kubectl get serviceaccount dashboard -o jsonpath=\"{.secrets[0].name}\") -o jsonpath=\"{.data.token}\" | base64 --decode",
            "title": "Optional - Enabling the Kubernetes dashboard"
        },
        {
            "location": "/k8s-deployment/#optional-allow-workers-to-run-on-head-node",
            "text": "You can now add additional nodes to the cluster, or \"taint\" the master to allow work to be scheduled onto it (single node configuration)  kubectl taint nodes --all node-role.kubernetes.io/master-",
            "title": "Optional - Allow workers to run on head node"
        },
        {
            "location": "/k8s-deployment/#adding-workers",
            "text": "Hopefully you saved the output of  kubeadm init . If so, you can just copy/paste the  kubeadm join ...  bits from the output into your worker node.  If not, here's how to regenerate it. On the master:  $ sudo kubeadm token create --print-join-command\nkubeadm join <master ip>:6443 --token <token> --discovery-token-ca-cert-hash sha256:<hash>  Just paste that into your worker.  Note that the token expires after 24h, after which you'll need to create a new token to add workers to your cluster .",
            "title": "Adding Workers"
        },
        {
            "location": "/k8s-deployment/#adding-namespaces-users-and-access-control",
            "text": "For all of the following, it's assumed that you'll save the yaml to a file and run  kubectl -f  against it.",
            "title": "Adding namespaces, users, and access control"
        },
        {
            "location": "/k8s-deployment/#creating-a-new-namespace",
            "text": "It's helpful to have users isolated to a separate namespace. Here's how to create one:  kind: Namespace\napiVersion: v1\nmetadata:\n    name: development\n    labels:\n        name: development",
            "title": "Creating a new namespace"
        },
        {
            "location": "/k8s-deployment/#create-a-new-user-context",
            "text": "The easiest way to add new users is to use kubeadm to generate a new client context:  # kubeadm alpha phase kubeconfig user --client-name=lincoln\napiVersion: v1\nclusters:\n- cluster:\n    certificate-authority-data: <base64-encoded data> \n    server: https://<your k8s API IP>:6443\n  name: kubernetes\ncontexts:\n- context:\n    cluster: kubernetes\n    user: lincoln\n  name: lincoln@kubernetes\ncurrent-context: lincoln@kubernetes\nkind: Config\npreferences: {}\nusers:\n- name: lincoln\n  user:\n    client-certificate-data: <base64 encoded data> \n    client-key-data: <base64 encoded data>   This is a  .kube/config  file that you should send to your non-admin user.",
            "title": "Create a new user context"
        },
        {
            "location": "/k8s-deployment/#create-roles-and-bind-them-to-the-user",
            "text": "Kubernetes uses a role-based access control (RBAC) system. To let users do anything, you'll need to \"bind\" a role to them.  Here's a role that allows a user to create deployments, replicasets, pods, and jobs:  kind: Role\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  namespace: development\n  name: deployment-manager\nrules:\n- apiGroups: [\"\", \"extensions\", \"apps\", \"batch\"]\n  resources: [\"deployments\", \"replicasets\", \"pods\", \"jobs\"]\n  verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\", \"delete\"]  And then to associate that role to our newly created user:  kind: RoleBinding\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: deployment-manager-binding\n  namespace: development\nsubjects:\n- kind: User\n  name: lincoln\n  apiGroup: \"\"\nroleRef:\n  kind: Role\n  name: deployment-manager\n  apiGroup: \"\"",
            "title": "Create roles and bind them to the user"
        },
        {
            "location": "/monitoring/",
            "text": "Monitoring deployment(s)\n\n\nMonitoring XCache operational parameters\n\n\nInformation from cache files is collected once per hour and sent to Elasticsearch at UChicago. \nBasic summary dashboard can be looked up here \nXCache state",
            "title": "Monitoring"
        },
        {
            "location": "/monitoring/#monitoring-deployments",
            "text": "",
            "title": "Monitoring deployment(s)"
        },
        {
            "location": "/monitoring/#monitoring-xcache-operational-parameters",
            "text": "Information from cache files is collected once per hour and sent to Elasticsearch at UChicago. \nBasic summary dashboard can be looked up here  XCache state",
            "title": "Monitoring XCache operational parameters"
        },
        {
            "location": "/analytics/",
            "text": "Simulating XCache on historical data\n\n\nRucio Traces\n\n\nRucio runs a REST API that collects data from diffent clients requesting the data.\nThis can be a user trying to download a file or dataset or Pilot trying to get path to data.\nRecords are described \nhere\n.\n\n\nTo destinguish type of access one uses field \"eventType\". The most important are:\n\n get_sm - production job input download\n\n get_sm_a - analysis job input download\n\n put_sm - production job output upload\n\n put_sm_a - analysis job output upload   \n\n\nResults\n\n\n\n\nunderstanding formats, simulation of single site \nslides\n.\n\n\nunderstanding multilayer cache, throughput needed, simulation of US network \nslides\n.\n\n\nsimulation of cache optimized job scheduling.\n\n\n\n\nAB testing\n\n\n\n\neffect on jobs wall times\n\n\neffect on tasks turn-around times",
            "title": "Analytics"
        },
        {
            "location": "/analytics/#simulating-xcache-on-historical-data",
            "text": "",
            "title": "Simulating XCache on historical data"
        },
        {
            "location": "/analytics/#rucio-traces",
            "text": "Rucio runs a REST API that collects data from diffent clients requesting the data.\nThis can be a user trying to download a file or dataset or Pilot trying to get path to data.\nRecords are described  here .  To destinguish type of access one uses field \"eventType\". The most important are:  get_sm - production job input download  get_sm_a - analysis job input download  put_sm - production job output upload  put_sm_a - analysis job output upload",
            "title": "Rucio Traces"
        },
        {
            "location": "/analytics/#results",
            "text": "understanding formats, simulation of single site  slides .  understanding multilayer cache, throughput needed, simulation of US network  slides .  simulation of cache optimized job scheduling.",
            "title": "Results"
        },
        {
            "location": "/analytics/#ab-testing",
            "text": "effect on jobs wall times  effect on tasks turn-around times",
            "title": "AB testing"
        },
        {
            "location": "/about/",
            "text": "XCache docs v.01\n\n\nIlija Vukotic (ivukotic@cern.ch)",
            "title": "About"
        },
        {
            "location": "/about/#xcache-docs-v01",
            "text": "Ilija Vukotic (ivukotic@cern.ch)",
            "title": "XCache docs v.01"
        }
    ]
}